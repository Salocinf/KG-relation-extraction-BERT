{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Installs and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zri8y-cbrqoM",
    "outputId": "92634eed-4b67-4078-9683-50260e79c82e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: wikitextparser in ./.local/lib/python3.9/site-packages (0.53.0)\n",
      "Requirement already satisfied: regex>=2022.9.11 in ./.local/lib/python3.9/site-packages (from wikitextparser) (2023.6.3)\n",
      "Requirement already satisfied: wcwidth in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from wikitextparser) (0.2.8)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: rdflib in ./.local/lib/python3.9/site-packages (7.0.0)\n",
      "Requirement already satisfied: isodate<0.7.0,>=0.6.0 in ./.local/lib/python3.9/site-packages (from rdflib) (0.6.1)\n",
      "Requirement already satisfied: pyparsing<4,>=2.1.0 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from rdflib) (3.1.1)\n",
      "Requirement already satisfied: six in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from isodate<0.7.0,>=0.6.0->rdflib) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in ./.local/lib/python3.9/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.local/lib/python3.9/site-packages (from nltk) (2023.6.3)\n",
      "Requirement already satisfied: tqdm in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from nltk) (4.66.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: wittgenstein in ./.local/lib/python3.9/site-packages (0.3.4)\n",
      "Requirement already satisfied: pandas in ./.local/lib/python3.9/site-packages (from wittgenstein) (2.1.1)\n",
      "Requirement already satisfied: numpy in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from wittgenstein) (1.26.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from pandas->wittgenstein) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from pandas->wittgenstein) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./.local/lib/python3.9/site-packages (from pandas->wittgenstein) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->wittgenstein) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: imbalanced-learn in ./.local/lib/python3.9/site-packages (0.11.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from imbalanced-learn) (1.26.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from imbalanced-learn) (1.11.3)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from imbalanced-learn) (1.3.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from imbalanced-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from imbalanced-learn) (3.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install wikitextparser\n",
    "!pip install rdflib\n",
    "!pip install nltk\n",
    "!pip install wittgenstein\n",
    "!pip install -U imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in ./testenv/lib/python3.9/site-packages (2.0.2)\n",
      "Requirement already satisfied: numpy in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from xgboost) (1.26.0)\n",
      "Requirement already satisfied: scipy in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from xgboost) (1.11.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "sHteH4rEtsg4",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/ma/ma_ma/ma_nfuerhau/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "from typing import Iterator, Optional, Union\n",
    "from enum import Enum\n",
    "import wikitextparser as wtp\n",
    "import bz2\n",
    "from lxml import etree\n",
    "import re\n",
    "import requests\n",
    "from rdflib import Graph, URIRef, RDF, RDFS\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import wittgenstein as lw\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "import time\n",
    "import sys\n",
    "import traceback\n",
    "import multiprocessing\n",
    "#import tensorflow as tf\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "# Ensure you have NLTK sentence tokenizer downloaded\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## WikiPageParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_cskWUXInPgs",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LOGGING\n",
    "def get_logger():\n",
    "    return logging.getLogger('impl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "FhVTJvNVmNjr",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WikiPageParser:\n",
    "    \"\"\"Parse WikiText as stream and return content based on page markers (only for simple article pages).\"\"\"\n",
    "    def __init__(self):\n",
    "        self.processed_pages = 0\n",
    "        self.page_markup = {}\n",
    "        self.title = None\n",
    "        self.namespace = None\n",
    "        self.tag_content = ''\n",
    "\n",
    "    def start(self, tag, _):\n",
    "        if tag.endswith('}page'):\n",
    "            self.title = None\n",
    "            self.namespace = None\n",
    "            self.processed_pages += 1\n",
    "            if self.processed_pages % 100000 == 0:\n",
    "                get_logger().debug(f'Parsed markup of {self.processed_pages} pages.')\n",
    "\n",
    "    def end(self, tag):\n",
    "        if tag.endswith('}title'):\n",
    "            self.title = self.tag_content.strip()\n",
    "        elif tag.endswith('}ns'):\n",
    "            self.namespace = self.tag_content.strip()\n",
    "        elif tag.endswith('}text') and self._valid_page():\n",
    "            self.page_markup[self.title] = self.tag_content.strip()\n",
    "        self.tag_content = ''\n",
    "\n",
    "    def data(self, chars):\n",
    "        self.tag_content += chars\n",
    "\n",
    "    def close(self) -> dict:\n",
    "        return self.page_markup\n",
    "\n",
    "    def _valid_page(self) -> bool:\n",
    "        return self.namespace in ['0', '10', '14']  # 0 = Page, 10 = Template, 14 = Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "SU7xHWYYtc3_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Namespace(Enum):\n",
    "    OWL = 'http://www.w3.org/2002/07/owl#'\n",
    "    WIKIPEDIA = 'http://en.wikipedia.org/wiki/'\n",
    "\n",
    "    PREFIX_TEMPLATE = 'Template:'\n",
    "    PREFIX_CATEGORY = 'Category:'\n",
    "    PREFIX_FILE = 'File:'\n",
    "    PREFIX_IMAGE = 'Image:'\n",
    "    PREFIX_LIST = 'List_of_'\n",
    "    PREFIX_LISTS = 'Lists_of_'\n",
    "    PREFIX_LISTCATEGORY = PREFIX_CATEGORY + PREFIX_LISTS\n",
    "\n",
    "    DBP_ONTOLOGY = 'http://dbpedia.org/ontology/'\n",
    "    DBP_RESOURCE = 'http://dbpedia.org/resource/'\n",
    "    DBP_TEMPLATE = DBP_RESOURCE + PREFIX_TEMPLATE\n",
    "    DBP_CATEGORY = DBP_RESOURCE + PREFIX_CATEGORY\n",
    "    DBP_FILE = DBP_RESOURCE + PREFIX_FILE\n",
    "    DBP_IMAGE = DBP_RESOURCE + PREFIX_IMAGE\n",
    "    DBP_LIST = DBP_RESOURCE + PREFIX_LIST\n",
    "\n",
    "    CLG_ONTOLOGY = 'http://caligraph.org/ontology/'\n",
    "    CLG_RESOURCE = 'http://caligraph.org/resource/'\n",
    "\n",
    "class RdfClass(Enum):\n",
    "    OWL_THING = 'http://www.w3.org/2002/07/owl#Thing'\n",
    "    OWL_CLASS = 'http://www.w3.org/2002/07/owl#Class'\n",
    "    OWL_NAMED_INDIVIDUAL = 'http://www.w3.org/2002/07/owl#NamedIndividual'\n",
    "    OWL_OBJECT_PROPERTY = 'http://www.w3.org/2002/07/owl#ObjectProperty'\n",
    "    OWL_DATATYPE_PROPERTY = 'http://www.w3.org/2002/07/owl#DatatypeProperty'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "pnfQtOIq3V_B",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def is_entity_name(name: str) -> bool:\n",
    "    invalid_prefixes = (Namespace.PREFIX_LIST.value, Namespace.PREFIX_FILE.value, Namespace.PREFIX_IMAGE.value,\n",
    "                        Namespace.PREFIX_CATEGORY.value, Namespace.PREFIX_TEMPLATE.value)\n",
    "    return name and not name.startswith(invalid_prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Ii3S5IGytm-_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def name2iri(name: str, prefix: Union[str, Enum]) -> str:\n",
    "    if name == 'Thing':\n",
    "        return RdfClass.OWL_THING.value\n",
    "    prefix = prefix.value if isinstance(prefix, Enum) else prefix\n",
    "    return prefix + name\n",
    "\n",
    "def name2resource_iri(name: str) -> str:\n",
    "    return name2iri(name, Namespace.DBP_RESOURCE)\n",
    "\n",
    "def label2name(label: str) -> str:\n",
    "    return label.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def capitalize(text: str) -> str:\n",
    "    return text[0].upper() + text[1:] if len(text) > 1 else text.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _remove_language_tag(link_target: str) -> str:\n",
    "    if not link_target or link_target[0] != ':':\n",
    "        return link_target\n",
    "    if len(link_target) < 4 or link_target[3] != ':':\n",
    "        return link_target[1:]\n",
    "    return link_target[4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_resource_name_for_wikilink(wikilink: wtp.WikiLink) -> Optional[str]:\n",
    "    return label2name(capitalize(_remove_language_tag(wikilink.target.strip())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Extract Abstract and Calculate Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "OXX3DxOv3qcV",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Own implementation\n",
    "\n",
    "def extract_abstract(wikipage: wtp.WikiText) -> str:\n",
    "  parsed_page = wtp.parse(wikipage)\n",
    "  try:\n",
    "    result = parsed_page.plain_text(replace_bolds_and_italics=True, replace_wikilinks=False).strip(\" '\\t\\n\")\n",
    "    result = re.sub(r'\\n+', '\\n', result)\n",
    "    result = re.sub(r' +', ' ', result)\n",
    "    result = result.split('==', 1)[0]\n",
    "    if result.split('\\n')[0].startswith('[[File'):\n",
    "      result = result.split('\\n')[1:][0]\n",
    "    return result\n",
    "  except(IndexError):\n",
    "\n",
    "    result = parsed_page.get_sections(level=0)[0].pformat()\n",
    "    result = result.split('\\n\\n')[1:]\n",
    "    result = \"\\n\".join(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "JVLhESvo5WSx",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Own implementation\n",
    "\n",
    "def extract_wikilinks(wikipage: wtp.WikiText) -> [str]:\n",
    "    abstract = extract_abstract(wikipage)\n",
    "    parsed_abstract = wtp.parse(abstract)\n",
    "    links = []\n",
    "    for link in parsed_abstract.wikilinks:\n",
    "        links.append(name2resource_iri(get_resource_name_for_wikilink(link)))\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example feature extraction function\n",
    "def extract_features(candidate_link, entities, candidates, sentence_idx: int) -> dict:\n",
    "    features = {}\n",
    "    \n",
    "    all_entities = []\n",
    "    for sentence in entities:\n",
    "        for l in sentence:\n",
    "            all_entities.append(l)\n",
    "            \n",
    "    all_candidates = []\n",
    "    for candidate in candidates:\n",
    "        all_candidates.append(candidate[0])\n",
    "        \n",
    "    candidates_in_sentence = [candidate[0] for candidate in candidates if candidate[1] == sentence_idx]\n",
    "\n",
    "    # Feature F00: Total number of entities in the abstract (all sentences)\n",
    "    features[\"F00\"] = len(candidates)\n",
    "    \n",
    "    # Feature F01: Number of candidates in the candidate's sentence\n",
    "    features[\"F01\"] = len(candidates_in_sentence)\n",
    "    \n",
    "    # Feature F02: Position of candidate w.r.t. all other candidates in the abstract\n",
    "    features[\"F02\"] = all_candidates.index(candidate_link)\n",
    "    \n",
    "    # Feature F03: Position of candidate w.r.t candidates in the sentence\n",
    "    features[\"F03\"] = candidates_in_sentence.index(candidate_link)\n",
    "    \n",
    "    # Feature F04: Number of entities in the sentence\n",
    "    features[\"F04\"] = len(entities[sentence_idx])\n",
    "    \n",
    "    # Feature F05: Position of candidate w.r.t all other entities in the abstract\n",
    "    features[\"F05\"] = all_entities.index(candidate_link)\n",
    "    \n",
    "    # Feature F06: Position of candidate w.r.t entities in sentence\n",
    "    features[\"F06\"] = entities[sentence_idx].index(candidate_link)\n",
    "    \n",
    "    # Feature F07: Position of candidates sentence in the abstract\n",
    "    features[\"F07\"] = sentence_idx #TODO: do I have to put + 1?\n",
    "    \n",
    "    features_scaled = {key:(1/(pow(2, value))) for (key, value) in features.items()}\n",
    "\n",
    "    return features_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Own implementation\n",
    "\n",
    "def get_domain_range_relation(dbpedia_ontology: Graph, relationURL: str) -> (str, str):\n",
    "  relation_uri = URIRef(relationURL)\n",
    "  domain = None\n",
    "  range = None\n",
    "\n",
    "  for s, p, o in dbpedia_ontology.triples((relation_uri, RDF.type, None)):\n",
    "      for s2, p2, o2 in dbpedia_ontology.triples((s, RDFS.domain, None)):\n",
    "        if o2:\n",
    "          domain = o2\n",
    "        else:\n",
    "          domain = None\n",
    "      for s2, p2, o2 in dbpedia_ontology.triples((s, RDFS.range, None)):\n",
    "        if o2:\n",
    "          range = o2\n",
    "        else:\n",
    "          range = None\n",
    "\n",
    "  return domain, range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Get abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "urls = [\n",
    "    'enwiki-20230901-pages-articles-multistream1.xml-p1p41242.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream2.xml-p41243p151573.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream3.xml-p151574p311329.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream4.xml-p311330p558391.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream5.xml-p558392p958045.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream6.xml-p958046p1483661.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream7.xml-p1483662p2134111.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream8.xml-p2134112p2936260.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream9.xml-p2936261p4045402.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream10.xml-p4045403p5399366.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream11.xml-p5399367p6899366.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream11.xml-p6899367p7054859.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream12.xml-p7054860p8554859.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream12.xml-p8554860p9172788.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream13.xml-p9172789p10672788.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream13.xml-p10672789p11659682.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream14.xml-p11659683p13159682.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream14.xml-p13159683p14324602.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream15.xml-p14324603p15824602.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream15.xml-p15824603p17324602.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream15.xml-p17324603p17460152.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream16.xml-p17460153p18960152.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream16.xml-p18960153p20460152.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream16.xml-p20460153p20570392.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream17.xml-p20570393p22070392.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream17.xml-p22070393p23570392.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream17.xml-p23570393p23716197.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream18.xml-p23716198p25216197.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream18.xml-p25216198p26716197.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream18.xml-p26716198p27121850.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream19.xml-p27121851p28621850.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream19.xml-p28621851p30121850.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream19.xml-p30121851p31308442.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream20.xml-p31308443p32808442.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream20.xml-p32808443p34308442.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream20.xml-p34308443p35522432.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream21.xml-p35522433p37022432.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream21.xml-p37022433p38522432.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream21.xml-p38522433p39996245.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream22.xml-p39996246p41496245.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream22.xml-p41496246p42996245.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream22.xml-p42996246p44496245.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream22.xml-p44496246p44788941.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream23.xml-p44788942p46288941.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream23.xml-p46288942p47788941.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream23.xml-p47788942p49288941.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream23.xml-p49288942p50564553.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream24.xml-p50564554p52064553.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream24.xml-p52064554p53564553.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream24.xml-p53564554p55064553.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream24.xml-p55064554p56564553.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream24.xml-p56564554p57025655.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream25.xml-p57025656p58525655.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream25.xml-p58525656p60025655.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream25.xml-p60025656p61525655.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream25.xml-p61525656p62585850.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream26.xml-p62585851p63975909.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream27.xml-p63975910p65475909.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream27.xml-p65475910p66975909.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream27.xml-p66975910p68475909.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream27.xml-p68475910p69975909.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream27.xml-p69975910p71475909.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream27.xml-p71475910p72975909.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream27.xml-p72975910p74475909.bz2',\n",
    "    'enwiki-20230901-pages-articles-multistream27.xml-p74475910p74725399.bz2'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pickle.load(open('/pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/dataPickle.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open('abstracts.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "id": "6OtjSXk7mSTq",
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test1 https://dumps.wikimedia.org/enwiki/20230901/enwiki-20230901-pages-articles-multistream1.xml-p1p41242.bz2\n",
      "Test2 https://dumps.wikimedia.org/enwiki/20230901/enwiki-20230901-pages-articles-multistream1.xml-p1p41242.bz2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 17198/27179 [1:16:56<44:39,  3.73it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m markup[:\u001b[38;5;241m9\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#REDIRECT\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     17\u001b[0m         data\u001b[38;5;241m.\u001b[39mupdate({name2resource_iri(label2name(page_title)): markup})\n\u001b[0;32m---> 18\u001b[0m         \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/dataPickle.p\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest3\u001b[39m\u001b[38;5;124m'\u001b[39m, url)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Get data directly from website\n",
    "\n",
    "data = {}\n",
    "\n",
    "parser = etree.XMLParser(target=WikiPageParser())\n",
    "for url in urls[:5]:\n",
    "    req = requests.get(url, stream=True)\n",
    "    with bz2.open(req.raw, 'rb') as dbp_pages_file:\n",
    "        try:\n",
    "            print('Test1', url)\n",
    "            page_markup = etree.parse(dbp_pages_file, parser)\n",
    "            print('Test2', url)\n",
    "        except(EOFError):\n",
    "            print(\"Problem with file\", url)\n",
    "            continue\n",
    "        for page_title, markup in tqdm(page_markup.items()):\n",
    "            if markup[:9] != '#REDIRECT':\n",
    "                data.update({name2resource_iri(label2name(page_title)): markup})\n",
    "                pickle.dump(data, open('/pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/dataPickle.p', 'wb'))\n",
    "        print('Test3', url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erledigt: enwiki-20230901-pages-articles-multistream27.xml-p68475910p69975909.bz2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [12:43<50:52, 763.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erledigt: enwiki-20230901-pages-articles-multistream27.xml-p69975910p71475909.bz2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [22:59<33:50, 676.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erledigt: enwiki-20230901-pages-articles-multistream27.xml-p71475910p72975909.bz2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [31:41<20:12, 606.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erledigt: enwiki-20230901-pages-articles-multistream27.xml-p72975910p74475909.bz2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [39:25<09:09, 549.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erledigt: enwiki-20230901-pages-articles-multistream27.xml-p74475910p74725399.bz2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [41:57<00:00, 503.58s/it]\n"
     ]
    }
   ],
   "source": [
    "# Get data from workspace\n",
    "\n",
    "data = {}\n",
    "\n",
    "parser = etree.XMLParser(target=WikiPageParser())\n",
    "\n",
    "prefix = '/pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/files/'\n",
    "\n",
    "for url in tqdm(urls[60:]):\n",
    "    complete_url = prefix + url\n",
    "    with bz2.open(complete_url, 'rb') as dbp_pages_file:\n",
    "        page_markup = etree.parse(dbp_pages_file, parser)\n",
    "        for page_title, markup in page_markup.items():\n",
    "            if markup[:9] != '#REDIRECT':\n",
    "                data.update({name2resource_iri(label2name(page_title)): markup})\n",
    "        print('Erledigt:', url)\n",
    "        pickle.dump(data, open('/pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/dataPickle.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "abstract_links = pickle.load(open('abstractLinksComplete.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract sentence structure\n",
    "\n",
    "def get_abstract_links(key, value):\n",
    "    try:\n",
    "        # Extract the abstract\n",
    "        abstract = extract_abstract(value)\n",
    "\n",
    "        # Split the abstract into sentences\n",
    "        sentences = sent_tokenize(abstract)\n",
    "\n",
    "        # Extract wikilinks for each sentence\n",
    "        sentence_links = [[(name2resource_iri(get_resource_name_for_wikilink(link))) for link in wtp.parse(sentence).wikilinks] for sentence in sentences]\n",
    "\n",
    "        # Store the sentence-level links in the abstract_links dictionary\n",
    "        return sentence_links\n",
    "    except IndexError:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare method for parallel processing\n",
    "\n",
    "prefix = '/pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/files/'\n",
    "folder = '/pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/abstract_links/'\n",
    "\n",
    "parser = etree.XMLParser(target=WikiPageParser())\n",
    "\n",
    "def process_url(url):\n",
    "    abstract_links = {}\n",
    "    filename = folder + url + '.p'\n",
    "    try:\n",
    "        complete_url = prefix + url\n",
    "        with bz2.open(complete_url, 'rb') as dbp_pages_file:\n",
    "            page_markup = etree.parse(dbp_pages_file, parser)\n",
    "            for page_title, markup in page_markup.items():\n",
    "                if not markup.startswith('#REDIRECT'):\n",
    "                    key = name2resource_iri(label2name(page_title))\n",
    "                    try:\n",
    "                        # Extract the abstract\n",
    "                        abstract = extract_abstract(markup)\n",
    "                        sentences = sent_tokenize(abstract)\n",
    "                        sentence_links = [[name2resource_iri(get_resource_name_for_wikilink(link)) for link in wtp.parse(sentence).wikilinks] for sentence in sentences]\n",
    "                        abstract_links[key] = sentence_links\n",
    "                    except IndexError:\n",
    "                        print('Has IndexError:', key)\n",
    "                        continue\n",
    "            pickle.dump(abstract_links, open(filename, 'wb'))\n",
    "            print('Nach dump', url)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nach dump enwiki-20230901-pages-articles-multistream18.xml-p26716198p27121850.bz2\n",
      "Nach dump enwiki-20230901-pages-articles-multistream13.xml-p10672789p11659682.bz2\n",
      "'NoneType' object has no attribute 'start'\n",
      "Nach dump enwiki-20230901-pages-articles-multistream20.xml-p34308443p35522432.bz2\n",
      "'NoneType' object has no attribute 'end'\n",
      "Nach dump enwiki-20230901-pages-articles-multistream23.xml-p44788942p46288941.bz2\n",
      "Nach dump enwiki-20230901-pages-articles-multistream19.xml-p28621851p30121850.bz2\n",
      "Nach dump\n",
      " enwiki-20230901-pages-articles-multistream19.xml-p30121851p31308442.bz2Nach dumpenwiki-20230901-pages-articles-multistream1.xml-p1p41242.bz2 \n",
      "Nach dump enwiki-20230901-pages-articles-multistream15.xml-p15824603p17324602.bz2\n",
      "Nach dump enwiki-20230901-pages-articles-multistream14.xml-p13159683p14324602.bz2\n",
      "Nach dump enwiki-20230901-pages-articles-multistream13.xml-p9172789p10672788.bz2\n",
      "Nach dump enwiki-20230901-pages-articles-multistream16.xml-p17460153p18960152.bz2\n",
      "Nach dump enwiki-20230901-pages-articles-multistream16.xml-p18960153p20460152.bz2\n",
      "Nach dump enwiki-20230901-pages-articles-multistream18.xml-p25216198p26716197.bz2\n",
      "Nach dump enwiki-20230901-pages-articles-multistream17.xml-p20570393p22070392.bz2\n",
      "Nach dump enwiki-20230901-pages-articles-multistream20.xml-p32808443p34308442.bz2\n",
      "Nach dump enwiki-20230901-pages-articles-multistream15.xml-p14324603p15824602.bz2\n",
      "Nach dump enwiki-20230901-pages-articles-multistream21.xml-p38522433p39996245.bz2\n",
      "Nach dump enwiki-20230901-pages-articles-multistream21.xml-p37022433p38522432.bz2\n",
      "Nach dump enwiki-20230901-pages-articles-multistream17.xml-p22070393p23570392.bz2\n",
      "Nach dump enwiki-20230901-pages-articles-multistream18.xml-p23716198p25216197.bz2\n",
      "Nach dump enwiki-20230901-pages-articles-multistream20.xml-p31308443p32808442.bz2\n",
      "Nach dump enwiki-20230901-pages-articles-multistream22.xml-p39996246p41496245.bz2\n",
      "Nach dump enwiki-20230901-pages-articles-multistream22.xml-p42996246p44496245.bz2\n",
      "Nach dump enwiki-20230901-pages-articles-multistream21.xml-p35522433p37022432.bz2\n",
      "Nach dump enwiki-20230901-pages-articles-multistream23.xml-p46288942p47788941.bz2\n",
      "Nach dump enwiki-20230901-pages-articles-multistream3.xml-p151574p311329.bz2\n",
      "Nach dump enwiki-20230901-pages-articles-multistream14.xml-p11659683p13159682.bz2\n",
      "Nach dump enwiki-20230901-pages-articles-multistream2.xml-p41243p151573.bz2\n",
      "Nach dump enwiki-20230901-pages-articles-multistream12.xml-p7054860p8554859.bz2\n",
      "Nach dump enwiki-20230901-pages-articles-multistream23.xml-p47788942p49288941.bz2\n",
      "'NoneType' object has no attribute 'start'\n",
      "Nach dump enwiki-20230901-pages-articles-multistream7.xml-p1483662p2134111.bz2\n",
      "Nach dump enwiki-20230901-pages-articles-multistream4.xml-p311330p558391.bz2\n",
      "Nach dump enwiki-20230901-pages-articles-multistream10.xml-p4045403p5399366.bz2\n",
      "Nach dump enwiki-20230901-pages-articles-multistream5.xml-p558392p958045.bz2\n",
      "'NoneType' object has no attribute 'end'\n",
      "Nach dump enwiki-20230901-pages-articles-multistream11.xml-p5399367p6899366.bz2\n",
      "Nach dump enwiki-20230901-pages-articles-multistream23.xml-p49288942p50564553.bz2\n",
      "Nach dump enwiki-20230901-pages-articles-multistream8.xml-p2134112p2936260.bz2\n",
      "Nach dump enwiki-20230901-pages-articles-multistream6.xml-p958046p1483661.bz2\n",
      "Nach dump enwiki-20230901-pages-articles-multistream9.xml-p2936261p4045402.bz2\n",
      "Nach dump enwiki-20230901-pages-articles-multistream24.xml-p56564554p57025655.bz2\n",
      "Nach dump enwiki-20230901-pages-articles-multistream24.xml-p50564554p52064553.bz2\n",
      "Nach dump enwiki-20230901-pages-articles-multistream25.xml-p61525656p62585850.bz2\n",
      "Nach dump enwiki-20230901-pages-articles-multistream24.xml-p53564554p55064553.bz2\n",
      "Nach dump enwiki-20230901-pages-articles-multistream25.xml-p58525656p60025655.bz2\n",
      "Nach dump enwiki-20230901-pages-articles-multistream25.xml-p57025656p58525655.bz2\n",
      "Nach dump enwiki-20230901-pages-articles-multistream25.xml-p60025656p61525655.bz2\n",
      "Nach dump enwiki-20230901-pages-articles-multistream26.xml-p62585851p63975909.bz2\n",
      "Nach dump enwiki-20230901-pages-articles-multistream27.xml-p72975910p74475909.bz2\n",
      "Nach dump enwiki-20230901-pages-articles-multistream27.xml-p63975910p65475909.bz2\n",
      "Nach dump enwiki-20230901-pages-articles-multistream27.xml-p71475910p72975909.bz2\n",
      "Nach dump enwiki-20230901-pages-articles-multistream27.xml-p65475910p66975909.bz2\n",
      "Nach dump enwiki-20230901-pages-articles-multistream27.xml-p69975910p71475909.bz2\n",
      "Nach dump enwiki-20230901-pages-articles-multistream27.xml-p68475910p69975909.bz2\n",
      "Nach dump enwiki-20230901-pages-articles-multistream27.xml-p66975910p68475909.bz2\n"
     ]
    }
   ],
   "source": [
    "urls_to_process = urls\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(40) as executor:\n",
    "    executor.map(process_url, urls_to_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open('dataPickle.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vor parsing: enwiki-20230901-pages-articles-multistream10.xml-p4045403p5399366.bz2\n"
     ]
    }
   ],
   "source": [
    "abstract_links_test = {}\n",
    "        \n",
    "parser = etree.XMLParser(target=WikiPageParser())\n",
    "\n",
    "prefix = '/pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/files/'\n",
    "#prefix = 'https://dumps.wikimedia.org/enwiki/20230901/'\n",
    "\n",
    "for url in urls[9:]:\n",
    "    data = {}\n",
    "    complete_url = prefix + url\n",
    "    #complete_url = requests.get(complete_url, stream=True).raw\n",
    "    print('Vor parsing:', url)\n",
    "    with bz2.open(complete_url, 'rb') as dbp_pages_file:\n",
    "        page_markup = etree.parse(dbp_pages_file, parser)\n",
    "        print('Nach parsing:', url)\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            futures = []\n",
    "            for key, value in tqdm(page_markup.items()):\n",
    "                futures.append(executor.submit(get_abstract_links, key, value))\n",
    "                abstract_links_test[key] = futures\n",
    "    print('Vor dump:', url)\n",
    "    #pickle.dump(data, open('/pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/abstractLinks.p', 'wb'))\n",
    "    pickle.dump(abstract_links, open('abstractLinksComplete.p', 'wb'))\n",
    "    print('Nach dump:', url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vor parsing: enwiki-20230901-pages-articles-multistream9.xml-p2936261p4045402.bz2\n",
      "Nach parsing: enwiki-20230901-pages-articles-multistream9.xml-p2936261p4045402.bz2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 373042/373042 [22:28<00:00, 276.65it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vor dump: enwiki-20230901-pages-articles-multistream9.xml-p2936261p4045402.bz2\n",
      "Nach dump: enwiki-20230901-pages-articles-multistream9.xml-p2936261p4045402.bz2\n",
      "Vor parsing: enwiki-20230901-pages-articles-multistream10.xml-p4045403p5399366.bz2\n",
      "Nach parsing: enwiki-20230901-pages-articles-multistream10.xml-p4045403p5399366.bz2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 154098/778031 [09:23<35:23, 293.88it/s]  "
     ]
    }
   ],
   "source": [
    "#abstract_links = {}\n",
    "\n",
    "parser = etree.XMLParser(target=WikiPageParser())\n",
    "\n",
    "prefix = '/pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/files/'\n",
    "#prefix = 'https://dumps.wikimedia.org/enwiki/20230901/'\n",
    "\n",
    "for url in urls[8:]:\n",
    "    data = {}\n",
    "    complete_url = prefix + url\n",
    "    #complete_url = requests.get(complete_url, stream=True).raw\n",
    "    print('Vor parsing:', url)\n",
    "    with bz2.open(complete_url, 'rb') as dbp_pages_file:\n",
    "        page_markup = etree.parse(dbp_pages_file, parser)\n",
    "        print('Nach parsing:', url)\n",
    "        for page_title, markup in tqdm(page_markup.items()):\n",
    "            if not markup.startswith('#REDIRECT'):\n",
    "                key = name2resource_iri(label2name(page_title))\n",
    "                try:\n",
    "                    # Extract the abstract\n",
    "                    abstract = extract_abstract(markup)\n",
    "                    sentences = sent_tokenize(abstract)\n",
    "                    sentence_links = [[(name2resource_iri(get_resource_name_for_wikilink(link))) for link in wtp.parse(sentence).wikilinks] for sentence in sentences]\n",
    "                    abstract_links[key] = sentence_links\n",
    "                except (IndexError):\n",
    "                    print('Has IndexError:', key)\n",
    "                    continue\n",
    "\n",
    "    print('Vor dump:', url)\n",
    "    #pickle.dump(data, open('/pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/abstractLinks.p', 'wb'))\n",
    "    pickle.dump(abstract_links, open('abstractLinksComplete.p', 'wb'))\n",
    "    print('Nach dump:', url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "epE89HwfqH9i",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clean data\n",
    "\n",
    "data_cleaned = {}\n",
    "\n",
    "for k, v in pickle.load(open('/pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/dataPickle.p', 'rb')).items():\n",
    "  if v[:9] != '#REDIRECT':\n",
    "    data_cleaned.update({k: v})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pickle.dump(data_cleaned, open('/pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/dataPickle.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Get transitive types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "A70EtKeaeCjs",
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = 'https://downloads.dbpedia.org/repo/dbpedia/mappings/instance-types/2022.12.01/instance-types_lang%3Den_transitive.ttl.bz2'\n",
    "req = requests.get(url, stream=True)\n",
    "encoded_types = []\n",
    "with bz2.open(req.raw, 'rb') as types:\n",
    "  pickle.dump(list(types), open('encodedTypes.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = pickle.load(open('encodedTypes.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "08NlOSB8XiTY",
    "tags": []
   },
   "outputs": [],
   "source": [
    "types = defaultdict(list)\n",
    "pattern = r'<([^>]+)>'\n",
    "for type in pickle.load(open('encodedTypes.p', 'rb')):\n",
    "  results = re.findall(pattern, type.decode('utf-8'))\n",
    "  if results[2].startswith('http://dbpedia.org/ontology/'):\n",
    "    types[results[0]].append(results[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pickle.dump(types, open('decodedTypes.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ky86apw9nS2u",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://dbpedia.org/ontology/Animal',\n",
       " 'http://dbpedia.org/ontology/Eukaryote',\n",
       " 'http://dbpedia.org/ontology/Person',\n",
       " 'http://dbpedia.org/ontology/Species']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "types = pickle.load(open('decodedTypes.p', 'rb'))\n",
    "types.get('http://dbpedia.org/resource/Abraham_Lincoln')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Get wikilinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://downloads.dbpedia.org/repo/dbpedia/generic/wikilinks/2022.12.01/wikilinks_lang%3Den.ttl.bz2'\n",
    "req = requests.get(url, stream=True)\n",
    "with bz2.open(req.raw, 'rb') as wikilinks:\n",
    "    pickle.dump(list(wikilinks), open('encodedWikilinks.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_wikilinks = pickle.load(open('encodedWikilinks.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_wikilinks = []\n",
    "pattern = r'<([^>]+)>'\n",
    "for triple in encoded_wikilinks:\n",
    "  results = re.findall(pattern, triple.decode('utf-8'))\n",
    "  decoded_wikilinks.append([results[0], results[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(decoded_wikilinks, open('decodedWikilinks.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wikilinks = pickle.load( open('decodedWikilinks.p', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Get mappingbased objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a6OtNrFuvThL",
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = 'https://downloads.dbpedia.org/repo/dbpedia/mappings/mappingbased-objects/2020.02.01/mappingbased-objects_lang%3Den.ttl.bz2'\n",
    "req = requests.get(url, stream=True)\n",
    "encoded_relations = []\n",
    "with bz2.open(req.raw, 'rb') as mappingbased_objects:\n",
    "    pickle.dump(list(mappingbased_objects), open('encodedRelations.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "lhgwgHoA7vH5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "relations = set()\n",
    "pattern = r'<([^>]+)>'\n",
    "for relation in pickle.load(open('encodedRelations.p', 'rb')):\n",
    "  results = re.findall(pattern, relation.decode('utf-8'))\n",
    "  relations.add(results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pickle.dump(relations, open('decodedRelations.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "zIqLLnDPw-VG",
    "tags": []
   },
   "outputs": [],
   "source": [
    "triples = []\n",
    "pattern = r'<([^>]+)>'\n",
    "for triple in pickle.load(open('encodedRelations.p', 'rb')):\n",
    "  results = re.findall(pattern, triple.decode('utf-8'))\n",
    "  triples.append([results[0], results[1], results[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pickle.dump(triples, open('triples.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 174
    },
    "id": "dPgr4sjMxsbS",
    "outputId": "1b3d43bc-4af1-4adb-8960-71a0311ad4e6",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(re.findall(pattern, encoded_relations[0].decode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "triples = pickle.load(open('triples.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "relation_domain_range = {}\n",
    "no_domain_range = []\n",
    "\n",
    "relations = pickle.load(open('decodedRelations.p', 'rb'))\n",
    "\n",
    "for relation in relations:\n",
    "    domain_range = get_domain_range_relation(dbpedia_ontology, relation)\n",
    "    if domain_range[0] or domain_range[1]:\n",
    "        relation_domain_range.update({relation: domain_range})\n",
    "    else:\n",
    "        no_domain_range.append(relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pickle.dump(relation_domain_range, open('relationDomainRange.p', 'wb'))\n",
    "pickle.dump(no_domain_range, open('noDomainRange.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "4GY__mUTL6cX",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_first_wikilink_resource(text: str) -> Optional[str]:\n",
    "    try:\n",
    "        for wl in wtp.parse(text).wikilinks:\n",
    "            res = get_resource_name_for_wikilink(wl)\n",
    "            if res is None:\n",
    "                continue\n",
    "            return res\n",
    "        return None\n",
    "    except (AttributeError, IndexError):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "id": "mNvhDs1V4dhK",
    "outputId": "ac6d6662-da69-4611-84b8-1d2088c4323e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "abstract_links = {}\n",
    "for key, value in data_cleaned.items():\n",
    "    try:\n",
    "        abstract_links.update({key: extract_wikilinks(value)})\n",
    "    except(IndexError):\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pickle.load(open('/pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/dataPickle.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def return_abstract_links(key, value):\n",
    "    try:\n",
    "        # Extract the abstract\n",
    "        abstract = extract_abstract(value)\n",
    "\n",
    "        # Split the abstract into sentences\n",
    "        sentences = sent_tokenize(abstract)\n",
    "\n",
    "        # Extract wikilinks for each sentence\n",
    "        sentence_links = [[(name2resource_iri(get_resource_name_for_wikilink(link))) for link in wtp.parse(sentence).wikilinks] for sentence in sentences]\n",
    "\n",
    "        # Store the sentence-level links in the abstract_links dictionary\n",
    "        #return key, sentence_links\n",
    "        abstract_links[key] = sentence_links\n",
    "    except IndexError:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "abstract_links = {}\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:  \n",
    "    for key, value in data.items():\n",
    "        executor.map(return_abstract_links, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(abstract_links, open('abstractLinksComplete.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to process each item in data for parallel processing\n",
    "def process_item(key, value):\n",
    "    try:\n",
    "        # Extract the abstract\n",
    "        abstract = extract_abstract(value)\n",
    "\n",
    "        # Split the abstract into sentences\n",
    "        sentences = sent_tokenize(abstract)\n",
    "\n",
    "        # Extract wikilinks for each sentence\n",
    "        sentence_links = [[(name2resource_iri(get_resource_name_for_wikilink(link))) for link in wtp.parse(sentence).wikilinks] for sentence in sentences]\n",
    "\n",
    "        return key, sentence_links\n",
    "    except IndexError:\n",
    "        print(key)\n",
    "        return key, []\n",
    "\n",
    "abstract_links_complete2 = {}\n",
    "\n",
    "num_processes = multiprocessing.cpu_count()\n",
    "\n",
    "# ThreadPoolExecutor with the specified number of processes\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=num_processes) as executor:\n",
    "    results = list(tqdm(executor.map(lambda item: process_item(*item), data.items()), total=len(data)))\n",
    "\n",
    "    # Collect the results\n",
    "    for key, sentence_links in results:\n",
    "        abstract_links_complete2[key] = sentence_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the abstract_links dictionary with sentence-level information\n",
    "abstract_links_complete2 = {}\n",
    "\n",
    "for key, value in tqdm(data.items()):\n",
    "    try:\n",
    "        # Extract the abstract\n",
    "        abstract = extract_abstract(value)\n",
    "\n",
    "        # Split the abstract into sentences\n",
    "        sentences = sent_tokenize(abstract)\n",
    "\n",
    "        # Extract wikilinks for each sentence\n",
    "        sentence_links = [[(name2resource_iri(get_resource_name_for_wikilink(link))) for link in wtp.parse(sentence).wikilinks] for sentence in sentences]\n",
    "\n",
    "        # Store the sentence-level links in the abstract_links dictionary\n",
    "        abstract_links_complete2[key] = sentence_links\n",
    "    except IndexError:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pickle.dump(abstract_links, open('abstractLinksComplete2.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enwiki-20230901-pages-articles-multistream19.xml-p27121851p28621850.bz2\n",
      "enwiki-20230901-pages-articles-multistream22.xml-p41496246p42996245.bz2\n"
     ]
    }
   ],
   "source": [
    "abstract_links_test = {}\n",
    "\n",
    "filepath = '/pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/abstract_links/'\n",
    "\n",
    "for url in urls:\n",
    "    try: \n",
    "        path = filepath + url + '.p'\n",
    "        file = pickle.load(open(path, 'rb'))\n",
    "        abstract_links_test.update(file)\n",
    "    except FileNotFoundError:\n",
    "        print(url)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pickle.dump(abstract_links_test, open('/pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/abstractLinksAll.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Candidates and Calculate Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_links = pickle.load(open('abstractLinks.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "candidates = {} # initialize dict for candidates\n",
    "\n",
    "for page, links in abstract_links.items(): # loop over all abstracts and their links\n",
    "    \n",
    "    page_type = types.get(page) # get the type of current page\n",
    "    \n",
    "    page_candidates = [] # initialize set for the candidates for current page\n",
    "    \n",
    "    for link in links: # loop over the links in an abstract\n",
    "        \n",
    "        link_type = types.get(link) # get the type of the page of the link\n",
    "        \n",
    "        if link_type: # if it has a type\n",
    "            \n",
    "            for relation in domain_range.items(): # loop over all relations and their domains/ranges\n",
    "                \n",
    "                if relation[1][0] == page_type or relation[1][1] == link_type: # if relation has corresponding domain or range\n",
    "                    \n",
    "                    page_candidates.append([page_type, relation[0], link_type]) # append the candidate with the relation\n",
    "                    \n",
    "    candidates.update({page: page_candidates}) # update the candidates dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "types = pickle.load(open('decodedTypes.p', 'rb'))\n",
    "abstract_links = pickle.load(open('abstractLinks.p', 'rb'))\n",
    "domain_range = pickle.load(open('relationDomainRange.p', 'rb'))\n",
    "triples = pickle.load(open('triples.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = pickle.load(open('decodedTypes.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_set = set()\n",
    "for object in types.values():\n",
    "    for type in object:\n",
    "        type_set.add(type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "abstract_links = pickle.load(open('abstractLinksComplete.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "abstract_links = pickle.load(open('/pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/abstractLinks.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pickle.dump(abstract_links_test, open('abstractLinks150k.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19808136/19808136 [01:38<00:00, 201415.56it/s]\n"
     ]
    }
   ],
   "source": [
    "relations_counts = {}\n",
    "\n",
    "for triple in tqdm(triples):\n",
    "    relation = triple[1]\n",
    "    \n",
    "    if relation in list(relations_counts.keys()):\n",
    "        relations_counts[relation] += 1\n",
    "    else:\n",
    "        relations_counts[relation] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "relation_counts_sorted = dict(sorted(relations_counts.items(), key=lambda x:x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pickle.dump(relation_counts_sorted, open('relationCountsSorted.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nicos_relations = [\n",
    "    'http://dbpedia.org/ontology/birthPlace',\n",
    "    'http://dbpedia.org/ontology/family',\n",
    "    'http://dbpedia.org/ontology/deathPlace',\n",
    "    'http://dbpedia.org/ontology/producer',\n",
    "    'http://dbpedia.org/ontology/writer',\n",
    "    'http://dbpedia.org/ontology/subsequentWork',\n",
    "    'http://dbpedia.org/ontology/previousWork',\n",
    "    'http://dbpedia.org/ontology/artist',\n",
    "    'http://dbpedia.org/ontology/formerTeam'  \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "abstract_links = pickle.load(open('/pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/abstractLinksAll.p', 'rb'))\n",
    "types = pickle.load(open('decodedTypes.p', 'rb'))\n",
    "domain_range = pickle.load(open('relationDomainRange.p', 'rb'))\n",
    "triples = pickle.load(open('triples.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19808136/19808136 [00:06<00:00, 3247749.69it/s]\n"
     ]
    }
   ],
   "source": [
    "triples_with_nicos_relations = []\n",
    "for triple in tqdm(triples):\n",
    "    if triple[1] in nicos_relations:\n",
    "        triples_with_nicos_relations.append(triple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10067593/10067593 [36:21<00:00, 4614.02it/s] \n"
     ]
    }
   ],
   "source": [
    "candidates_with_features = []  # Initialize dict for candidates with features\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for page, sentence_links in tqdm(abstract_links.items()):\n",
    "\n",
    "    page_types = types.get(page)\n",
    "    candidates = []\n",
    "    candidate_links = []\n",
    "\n",
    "    for sentence_idx, links in enumerate(sentence_links):\n",
    "        for entity in links:\n",
    "            entity_types = types.get(entity)\n",
    "\n",
    "            if entity_types:\n",
    "                for k, v in domain_range.items():\n",
    "\n",
    "                    if not page_types or not v[1]:\n",
    "                        continue\n",
    "\n",
    "                    elif str(v[0]) in page_types and str(v[1]) in entity_types and k in nicos_relations:\n",
    "                        candidate = [page, k, entity, sentence_idx]\n",
    "\n",
    "                        candidates.append(candidate)\n",
    "                        candidate_links.append([entity, sentence_idx])\n",
    "\n",
    "    interim = candidate_links\n",
    "    candidate_links = []\n",
    "\n",
    "    [candidate_links.append(link) for link in interim if link not in candidate_links] \n",
    "\n",
    "    for candidate in candidates:\n",
    "        candidate_features = extract_features(candidate[2], sentence_links, candidate_links, candidate[3])\n",
    "\n",
    "        X.append(list(candidate_features.values()))\n",
    "        candidates_with_features.append([candidate[0], candidate[1], candidate[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pickle.dump(X, open('/pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/X2.p', 'wb'))\n",
    "pickle.dump(candidates_with_features, open('/pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/candidatesWithFeatures2.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = pickle.load(open('/pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/X2.p', 'rb'))\n",
    "candidates_with_features = pickle.load(open('/pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/candidatesWithFeatures2.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(candidates_with_features, open('candidates.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = pickle.load(open('decodedTypes.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pickle.load(open('dataFrameAll.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_set = pickle.load(open('typeSet.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_wikilinks = pickle.load(open('decodedWikilinks.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_wikilinks_tuples = {tuple(list) for list in decoded_wikilinks}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikilink_set = pickle.load(open('/pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/wikilinkSet.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikilink_set = pickle.load(open('wikilinkSet.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tqdm for monitoring the progress\n",
    "pbar = tqdm(total=len(df_previouswork))\n",
    "\n",
    "# Iterate through the DataFrame and populate F08\n",
    "for index, row in df_bp2.iterrows():\n",
    "    if (row['range'], row['domain']) in decoded_wikilinks:\n",
    "        df_bp2.at[index, 'F08'] = 1\n",
    "    pbar.update(1)  # Update progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import partial\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 17718/50000 [00:00<00:02, 15654.81it/s]IOStream.flush timed out\n",
      "100%|█████████▉| 49836/50000 [00:34<00:00, 17133.19it/s]/scratch/slurm_tmpdir/job_22726978/ipykernel_616577/1749030231.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_formerteam['F08'] = results\n",
      "100%|██████████| 50000/50000 [00:34<00:00, 1439.35it/s] \n"
     ]
    }
   ],
   "source": [
    "# Initialize tqdm for monitoring the progress\n",
    "pbar = tqdm(total=len(df_formerteam))\n",
    "\n",
    "def check_and_update(row, wikilink_set, pbar):\n",
    "    if (row.range, row.domain) in wikilink_set:\n",
    "        result = 1\n",
    "    else:\n",
    "        result = 0\n",
    "\n",
    "    pbar.update(1)\n",
    "    return result\n",
    "\n",
    "# Parallelize the lookup and update\n",
    "results = []\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    for result in executor.map(partial(check_and_update, wikilink_set=wikilink_set, pbar=pbar), df_formerteam.itertuples(index=False)):\n",
    "        results.append(result)\n",
    "\n",
    "df_formerteam['F08'] = results\n",
    "\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:05<00:00, 8634.62it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize tqdm for monitoring the progress\n",
    "pbar = tqdm(total=len(df_formerteam))\n",
    "\n",
    "# Iterate through the DataFrame and populate the binary columns\n",
    "for index, row in df_formerteam.iterrows():\n",
    "    domain = row['domain']\n",
    "    for type in types.get(domain, []):\n",
    "        t = type.removeprefix('http://dbpedia.org/ontology/')\n",
    "        t = 'T:' + t\n",
    "        df_formerteam.at[index, t] = 1\n",
    "    pbar.update(1)  # Update progress bar\n",
    "    \n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(df_formerteam, open('data/dfFormerTeam50k', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(df2, open('df2.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "columns = ['domain', 'relation', 'range', 'F00', 'F01', 'F02', 'F03', 'F04', 'F05', 'F06', 'F07']\n",
    "\n",
    "df1 = pd.DataFrame(candidates_with_features, columns=['domain', 'relation', 'range'])\n",
    "df2 = pd.DataFrame(X, columns=['F00', 'F01', 'F02', 'F03', 'F04', 'F05', 'F06', 'F07'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>relation</th>\n",
       "      <th>range</th>\n",
       "      <th>F00</th>\n",
       "      <th>F01</th>\n",
       "      <th>F02</th>\n",
       "      <th>F03</th>\n",
       "      <th>F04</th>\n",
       "      <th>F05</th>\n",
       "      <th>F06</th>\n",
       "      <th>F07</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://dbpedia.org/resource/Abraham_Lincoln</td>\n",
       "      <td>http://dbpedia.org/ontology/birthPlace</td>\n",
       "      <td>http://dbpedia.org/resource/Union_(American_Ci...</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://dbpedia.org/resource/Abraham_Lincoln</td>\n",
       "      <td>http://dbpedia.org/ontology/deathPlace</td>\n",
       "      <td>http://dbpedia.org/resource/Union_(American_Ci...</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://dbpedia.org/resource/Abraham_Lincoln</td>\n",
       "      <td>http://dbpedia.org/ontology/birthPlace</td>\n",
       "      <td>http://dbpedia.org/resource/Kentucky</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://dbpedia.org/resource/Abraham_Lincoln</td>\n",
       "      <td>http://dbpedia.org/ontology/deathPlace</td>\n",
       "      <td>http://dbpedia.org/resource/Kentucky</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://dbpedia.org/resource/Abraham_Lincoln</td>\n",
       "      <td>http://dbpedia.org/ontology/birthPlace</td>\n",
       "      <td>http://dbpedia.org/resource/Indiana</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        domain  \\\n",
       "0  http://dbpedia.org/resource/Abraham_Lincoln   \n",
       "1  http://dbpedia.org/resource/Abraham_Lincoln   \n",
       "2  http://dbpedia.org/resource/Abraham_Lincoln   \n",
       "3  http://dbpedia.org/resource/Abraham_Lincoln   \n",
       "4  http://dbpedia.org/resource/Abraham_Lincoln   \n",
       "\n",
       "                                 relation  \\\n",
       "0  http://dbpedia.org/ontology/birthPlace   \n",
       "1  http://dbpedia.org/ontology/deathPlace   \n",
       "2  http://dbpedia.org/ontology/birthPlace   \n",
       "3  http://dbpedia.org/ontology/deathPlace   \n",
       "4  http://dbpedia.org/ontology/birthPlace   \n",
       "\n",
       "                                               range       F00   F01   F02  \\\n",
       "0  http://dbpedia.org/resource/Union_(American_Ci...  0.000977  0.50  1.00   \n",
       "1  http://dbpedia.org/resource/Union_(American_Ci...  0.000977  0.50  1.00   \n",
       "2               http://dbpedia.org/resource/Kentucky  0.000977  0.25  0.50   \n",
       "3               http://dbpedia.org/resource/Kentucky  0.000977  0.25  0.50   \n",
       "4                http://dbpedia.org/resource/Indiana  0.000977  0.25  0.25   \n",
       "\n",
       "   F03       F04       F05    F06   F07  \n",
       "0  1.0  0.015625  0.250000  1.000  0.50  \n",
       "1  1.0  0.015625  0.250000  1.000  0.50  \n",
       "2  1.0  0.062500  0.001953  0.500  0.25  \n",
       "3  1.0  0.062500  0.001953  0.500  0.25  \n",
       "4  0.5  0.062500  0.000488  0.125  0.25  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df1, df2], axis=1, join='inner')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_full = pickle.load(open('y_full.p', 'rb'))\n",
    "result_df = pd.DataFrame(y_full, columns=['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, result_df], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pickle.load(open('dataFrameAll.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample dataframes of size 50,000 with 10,000 1 labels\n",
    "\n",
    "def sample_dataframe(df, n, ratio):\n",
    "  # Filter rows with label '1' and '0'\n",
    "  label_1_rows = df[df['label'] == 1]\n",
    "  label_0_rows = df[df['label'] == 0]\n",
    "\n",
    "  if len(label_1_rows) >= (n * ratio):\n",
    "\n",
    "    # Select 10,000 rows with label '1' and 40,000 rows with label '0'\n",
    "    selected_rows_label_1 = label_1_rows.sample(n=int((n*ratio)), random_state=42)\n",
    "    selected_rows_label_0 = label_0_rows.sample(n=int((n*(1-ratio))), random_state=42)\n",
    "\n",
    "  else:\n",
    "    selected_rows_label_1 = label_1_rows\n",
    "    selected_rows_label_0 = label_0_rows.sample(n=(50000-len(label_1_rows)), random_state=42)\n",
    "\n",
    "  # Concatenate the selected rows\n",
    "  selected_rows = pd.concat([selected_rows_label_1, selected_rows_label_0])\n",
    "\n",
    "  # Shuffle the selected rows\n",
    "  return selected_rows.sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_birthplace = sample_dataframe(df[df.relation == 'http://dbpedia.org/ontology/birthPlace'], 50000, 0.2)\n",
    "df_family = sample_dataframe(df[df.relation == 'http://dbpedia.org/ontology/family'], 50000, 0.2)\n",
    "df_deathplace = sample_dataframe(df[df.relation == 'http://dbpedia.org/ontology/deathPlace'], 50000, 0.2)\n",
    "df_producer = sample_dataframe(df[df.relation == 'http://dbpedia.org/ontology/producer'], 50000, 0.2)\n",
    "df_writer = sample_dataframe(df[df.relation == 'http://dbpedia.org/ontology/writer'], 50000, 0.2)\n",
    "df_subsequentwork = sample_dataframe(df[df.relation == 'http://dbpedia.org/ontology/subsequentWork'], 50000, 0.2)\n",
    "df_previouswork = sample_dataframe(df[df.relation == 'http://dbpedia.org/ontology/previousWork'], 50000, 0.2)\n",
    "df_artist = sample_dataframe(df[df.relation == 'http://dbpedia.org/ontology/artist'], 50000, 0.2)\n",
    "df_formerteam = sample_dataframe(df[df.relation == 'http://dbpedia.org/ontology/formerTeam'], 50000, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations = [df_birthplace, df_family, df_deathplace, df_producer, df_writer, df_subsequentwork, \n",
    "             df_previouswork, df_artist, df_formerteam]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relations = []\n",
    "\n",
    "for relation in nicos_relations:\n",
    "    if not df[df.relation == relation].empty:\n",
    "        df_relations.append(df[df.relation == relation])\n",
    "    else:\n",
    "        print(relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "candidates = [\n",
    "    candidates_with_features[:10000],\n",
    "    candidates_with_features[10000:20000],\n",
    "    candidates_with_features[20000:30000],\n",
    "    candidates_with_features[30000:40000],\n",
    "    candidates_with_features[40000:50000],\n",
    "]\n",
    "\n",
    "y_full = []\n",
    "\n",
    "for i, candidate_set in enumerate(candidates):\n",
    "    y = []\n",
    "    for candidate in tqdm(candidate_set):\n",
    "        if candidate in triples_with_top_relations:\n",
    "            y.append(1)\n",
    "        else:\n",
    "            y.append(0)\n",
    "    y_full.append(i, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "candidates = [candidates_with_features[i:i + 3000] for i in range(0, len(candidates_with_features), 3000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_candidate_set(candidate_set):\n",
    "    try:\n",
    "        y = []\n",
    "        path = '/pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/'\n",
    "        name = path + str(candidates.index(candidate_set)) + '.p'\n",
    "        print('Processing', name)\n",
    "        for candidate in candidate_set:\n",
    "            if candidate in triples_with_nicos_relations:\n",
    "                y.append(1)\n",
    "            else:\n",
    "                y.append(0)\n",
    "        pickle.dump(y, open(name, 'wb'))\n",
    "        print('After dump', name)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1480.p\n",
      "Processing /pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1481.p\n",
      "ProcessingProcessing  /pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1483.p/pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1482.p\n",
      "\n",
      "Processing /pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1484.p\n",
      "Processing /pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1485.p\n",
      "Processing /pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1486.p\n",
      "Processing /pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1487.p\n",
      "Processing /pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1488.p\n",
      "ProcessingProcessingProcessingProcessingProcessingProcessingProcessing       /pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1495.p/pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1494.p/pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1493.p/pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1490.p/pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1489.p/pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1492.p/pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1491.p\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ProcessingProcessing  Processing/pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1497.p/pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1496.p \n",
      "\n",
      "/pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1498.p\n",
      "Processing /pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1499.p\n",
      "After dump\n",
      " /pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1480.pProcessing /pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1500.p\n",
      "After dump /pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1483.p\n",
      "Processing /pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1501.p\n",
      "After dump /pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1484.p\n",
      "Processing /pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1502.p\n",
      "After dump /pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1488.p\n",
      "Processing /pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1503.p\n",
      "After dump /pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1487.p\n",
      "Processing /pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1504.p\n",
      "After dump /pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1486.p\n",
      "Processing /pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1505.p\n",
      "After dump /pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1485.p\n",
      "Processing /pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1506.p\n",
      "After dump /pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1482.p\n",
      "Processing /pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1507.p\n",
      "After dump /pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1481.p\n",
      "Processing /pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1508.p\n",
      "After dump /pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1491.p\n",
      "Processing /pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1509.p\n",
      "After dump /pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1495.p\n",
      "Processing /pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1510.p\n",
      "After dump /pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1492.p\n",
      "Processing /pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1511.p\n",
      "After dump /pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1489.p\n",
      "Processing /pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1512.p\n",
      "After dump /pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1490.p\n",
      "Processing /pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1513.p\n",
      "After dump /pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1494.p\n",
      "Processing /pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1514.p\n",
      "After dump /pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1493.p\n",
      "Processing /pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1515.p\n",
      "After dump /pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1497.p\n",
      "Processing /pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1516.p\n",
      "After dump /pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1498.p\n",
      "Processing /pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1517.p\n",
      "After dump /pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1499.p\n",
      "Processing /pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1518.p\n",
      "After dump /pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1496.p\n",
      "Processing /pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/1519.p\n"
     ]
    }
   ],
   "source": [
    "# Create a ThreadPoolExecutor\n",
    "with concurrent.futures.ProcessPoolExecutor(20) as executor:\n",
    "    executor.map(process_candidate_set, candidates[1480:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_full = []\n",
    "path = \"/pfs/work7/workspace/scratch/ma_nfuerhau-masterthesis/y2/{index}.p\"\n",
    "\n",
    "for i in range(1524):\n",
    "    y_full += pickle.load(open(path.format(index=i), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(y_full, open('y_full2.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pickle.dump(X, open('X_full2.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to process a single candidate and return the label (1 or 0)\n",
    "def process_candidate(candidate, triples_with_top_relations):\n",
    "    if candidate in triples_with_top_relations:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "y = []\n",
    "\n",
    "# Use concurrent.futures to parallelize the processing\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    results = []\n",
    "\n",
    "    for candidate in tqdm(candidates_with_features[:10000]):\n",
    "        future = executor.submit(process_candidate, candidate, triples_with_top_relations)\n",
    "        results.append(future)\n",
    "\n",
    "    for result in results:\n",
    "        label = result.result()\n",
    "        y.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = pickle.load(open('y.p', 'rb'))\n",
    "y2 = pickle.load(open('y2.p', 'rb'))\n",
    "y3 = pickle.load(open('y3.p', 'rb'))\n",
    "y4 = pickle.load(open('y4.p', 'rb'))\n",
    "y5 = pickle.load(open('y5.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_full = pickle.load(open('X_full.p', 'rb'))\n",
    "y_full = pickle.load(open('y_full.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert X_full and y_full into a DataFrame\n",
    "data = pd.DataFrame(data=X_full, columns=['F00', 'F01', 'F02', 'F03', 'F04', 'F05', 'F06', 'F07'])\n",
    "data['target'] = y_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before balancing:\n",
      "label\n",
      "0    0.925525\n",
      "1    0.074475\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Class distribution after balancing:\n",
      "label\n",
      "0    0.5\n",
      "1    0.5\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.drop(['domain', 'relation', 'range', 'label'], axis=1),\n",
    "    df['label'],\n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Check the class distribution in the training data before balancing\n",
    "class_distribution_before = y_train.value_counts(normalize=True)\n",
    "print(\"Class distribution before balancing:\")\n",
    "print(class_distribution_before)\n",
    "\n",
    "# Balance the training data using RandomOverSampler\n",
    "oversampler = RandomOverSampler(sampling_strategy='minority', random_state=42)\n",
    "X_train_balanced, y_train_balanced = oversampler.fit_resample(X_train, y_train)\n",
    "\n",
    "# Check the class distribution in the training data after balancing\n",
    "class_distribution_after = y_train_balanced.value_counts(normalize=True)\n",
    "print(\"\\nClass distribution after balancing:\")\n",
    "print(class_distribution_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_birthplace.drop(['domain', 'relation', 'range', 'label'], axis=1),\n",
    "    df_birthplace['label'],\n",
    "    test_size=0.2,\n",
    "    shuffle=True,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\n",
    "    'data/dfBirthPlace50k.p',\n",
    "    'data/dfFamily50k.p',\n",
    "    'data/dfDeathPlace50k.p',\n",
    "    'data/dfProducer50k.p',\n",
    "    'data/dfWriter50k.p',\n",
    "    'data/dfSubsequentWork50k.p',\n",
    "    'data/dfPreviousWork50k.p',\n",
    "    'data/dfArtist50k.p',\n",
    "    'data/dfFormerTeam50k.p'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations = []\n",
    "\n",
    "for path in paths:\n",
    "    relations.append(pickle.load(open(path, 'rb')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Classifiers, train and test\n",
    "* Random Forest\n",
    "* Naive Bayes\n",
    "* RIPPER\n",
    "* SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb\n",
      "\n",
      "Relation: http://dbpedia.org/ontology/birthPlace\n",
      "Best Threshold: 0.999\n",
      "Best Precision: 0.20404636030906873\n",
      "Recall for best Threshold: 0.9950421417947447\n",
      "F1 for best Threshold: 0.33864844343204253\n",
      "\n",
      "Relation: http://dbpedia.org/ontology/family\n",
      "Best Threshold: 0.999\n",
      "Best Precision: 0.16324626865671643\n",
      "Recall for best Threshold: 1.0\n",
      "F1 for best Threshold: 0.2806736166800321\n",
      "\n",
      "Relation: http://dbpedia.org/ontology/deathPlace\n",
      "Best Threshold: 0.999\n",
      "Best Precision: 0.20549203756635362\n",
      "Recall for best Threshold: 0.9980168567178979\n",
      "F1 for best Threshold: 0.340810970964192\n",
      "\n",
      "Relation: http://dbpedia.org/ontology/producer\n",
      "Best Threshold: 0.999\n",
      "Best Precision: 0.12099772154934645\n",
      "Recall for best Threshold: 0.9970355731225297\n",
      "F1 for best Threshold: 0.21580579617153248\n",
      "\n",
      "Relation: http://dbpedia.org/ontology/writer\n",
      "Best Threshold: 0.546\n",
      "Best Precision: 0.6168831168831169\n",
      "Recall for best Threshold: 0.04709965294992563\n",
      "F1 for best Threshold: 0.08751727314601566\n",
      "\n",
      "Relation: http://dbpedia.org/ontology/subsequentWork\n",
      "Best Threshold: 0.999\n",
      "Best Precision: 0.06183392539964476\n",
      "Recall for best Threshold: 1.0\n",
      "F1 for best Threshold: 0.11646628332462101\n",
      "\n",
      "Relation: http://dbpedia.org/ontology/previousWork\n",
      "Best Threshold: 0.999\n",
      "Best Precision: 0.10680145039006703\n",
      "Recall for best Threshold: 1.0\n",
      "F1 for best Threshold: 0.19299116449915618\n",
      "\n",
      "Relation: http://dbpedia.org/ontology/artist\n",
      "Best Threshold: 0.994\n",
      "Best Precision: 0.5719298245614035\n",
      "Recall for best Threshold: 0.08081308874566187\n",
      "F1 for best Threshold: 0.1416159860990443\n",
      "\n",
      "Relation: http://dbpedia.org/ontology/formerTeam\n",
      "Best Threshold: 0.921\n",
      "Best Precision: 0.20477157360406092\n",
      "Recall for best Threshold: 1.0\n",
      "F1 for best Threshold: 0.3399342715092273\n",
      "\n",
      "-------------------------\n",
      "rf\n",
      "\n",
      "Relation: http://dbpedia.org/ontology/birthPlace\n",
      "Best Threshold: 0.789\n",
      "Best Precision: 0.8387096774193549\n",
      "Recall for best Threshold: 0.012890431333663858\n",
      "F1 for best Threshold: 0.025390625\n",
      "\n",
      "Relation: http://dbpedia.org/ontology/family\n",
      "Best Threshold: 0.724\n",
      "Best Precision: 0.9479166666666666\n",
      "Recall for best Threshold: 0.6933333333333334\n",
      "F1 for best Threshold: 0.800880088008801\n",
      "\n",
      "Relation: http://dbpedia.org/ontology/deathPlace\n",
      "Best Threshold: 0.587\n",
      "Best Precision: 0.7777777777777778\n",
      "Recall for best Threshold: 0.0034705007436787306\n",
      "F1 for best Threshold: 0.006910167818361303\n",
      "\n",
      "Relation: http://dbpedia.org/ontology/producer\n",
      "Best Threshold: 0.692\n",
      "Best Precision: 0.9333333333333333\n",
      "Recall for best Threshold: 0.01383399209486166\n",
      "F1 for best Threshold: 0.027263875365141188\n",
      "\n",
      "Relation: http://dbpedia.org/ontology/writer\n",
      "Best Threshold: 0.582\n",
      "Best Precision: 0.9375\n",
      "Recall for best Threshold: 0.014873574615765989\n",
      "F1 for best Threshold: 0.029282576866764276\n",
      "\n",
      "Relation: http://dbpedia.org/ontology/subsequentWork\n",
      "Best Threshold: 0.269\n",
      "Best Precision: 0.6356589147286822\n",
      "Recall for best Threshold: 0.14721723518850988\n",
      "F1 for best Threshold: 0.239067055393586\n",
      "\n",
      "Relation: http://dbpedia.org/ontology/previousWork\n",
      "Best Threshold: 0.736\n",
      "Best Precision: 0.8205128205128205\n",
      "Recall for best Threshold: 0.03292181069958848\n",
      "F1 for best Threshold: 0.0633036597428289\n",
      "\n",
      "Relation: http://dbpedia.org/ontology/artist\n",
      "Best Threshold: 0.98\n",
      "Best Precision: 0.95\n",
      "Recall for best Threshold: 0.009419930589985127\n",
      "F1 for best Threshold: 0.018654884634266077\n",
      "\n",
      "Relation: http://dbpedia.org/ontology/formerTeam\n",
      "Best Threshold: 0.837\n",
      "Best Precision: 0.9466666666666667\n",
      "Recall for best Threshold: 0.03520079325731284\n",
      "F1 for best Threshold: 0.06787762906309751\n",
      "\n",
      "-------------------------\n",
      "svm\n",
      "\n",
      "Relation: http://dbpedia.org/ontology/birthPlace\n",
      "Best Threshold: 0.709\n",
      "Best Precision: 0.6\n",
      "Recall for best Threshold: 0.002974714923153198\n",
      "F1 for best Threshold: 0.005920078934385792\n",
      "\n",
      "Relation: http://dbpedia.org/ontology/family\n",
      "Best Threshold: 0.81\n",
      "Best Precision: 0.9464285714285714\n",
      "Recall for best Threshold: 0.5047619047619047\n",
      "F1 for best Threshold: 0.6583850931677019\n",
      "\n",
      "Relation: http://dbpedia.org/ontology/deathPlace\n",
      "Best Threshold: 0.21\n",
      "Best Precision: 0.25274725274725274\n",
      "Recall for best Threshold: 0.011403073872087258\n",
      "F1 for best Threshold: 0.021821631878557873\n",
      "\n",
      "Relation: http://dbpedia.org/ontology/producer\n",
      "Best Threshold: 0.101\n",
      "Best Precision: 0.1012\n",
      "Recall for best Threshold: 1.0\n",
      "F1 for best Threshold: 0.1837994914638576\n",
      "\n",
      "Relation: http://dbpedia.org/ontology/writer\n",
      "Best Threshold: 0.193\n",
      "Best Precision: 0.23312527217302947\n",
      "Recall for best Threshold: 0.7962320277640059\n",
      "F1 for best Threshold: 0.3606557377049181\n",
      "\n",
      "Relation: http://dbpedia.org/ontology/subsequentWork\n",
      "Best Threshold: 0.057\n",
      "Best Precision: 0.0557\n",
      "Recall for best Threshold: 1.0\n",
      "F1 for best Threshold: 0.105522402197594\n",
      "\n",
      "Relation: http://dbpedia.org/ontology/previousWork\n",
      "Best Threshold: 0.623\n",
      "Best Precision: 0.5984703632887189\n",
      "Recall for best Threshold: 0.3220164609053498\n",
      "F1 for best Threshold: 0.41872909698996663\n",
      "\n",
      "Relation: http://dbpedia.org/ontology/artist\n",
      "Best Threshold: 0.918\n",
      "Best Precision: 0.875\n",
      "Recall for best Threshold: 0.0034705007436787306\n",
      "F1 for best Threshold: 0.00691358024691358\n",
      "\n",
      "Relation: http://dbpedia.org/ontology/formerTeam\n",
      "Best Threshold: 0.94\n",
      "Best Precision: 0.9\n",
      "Recall for best Threshold: 0.008924144769459593\n",
      "F1 for best Threshold: 0.017673048600883652\n",
      "\n",
      "-------------------------\n",
      "ripper\n",
      "\n",
      "Relation: http://dbpedia.org/ontology/birthPlace\n",
      "Best Threshold: 0.54\n",
      "Best Precision: 0.5513513513513514\n",
      "Recall for best Threshold: 0.20228061477441744\n",
      "F1 for best Threshold: 0.2959738846572361\n",
      "\n",
      "Relation: http://dbpedia.org/ontology/family\n",
      "Best Threshold: 0.871\n",
      "Best Precision: 0.948220064724919\n",
      "Recall for best Threshold: 0.5580952380952381\n",
      "F1 for best Threshold: 0.7026378896882493\n",
      "\n",
      "Relation: http://dbpedia.org/ontology/deathPlace\n",
      "Best Threshold: 0.637\n",
      "Best Precision: 0.6271186440677966\n",
      "Recall for best Threshold: 0.03668815071888944\n",
      "F1 for best Threshold: 0.06932084309133489\n",
      "\n",
      "Relation: http://dbpedia.org/ontology/producer\n",
      "Best Threshold: 0.64\n",
      "Best Precision: 0.6666666666666666\n",
      "Recall for best Threshold: 0.009881422924901186\n",
      "F1 for best Threshold: 0.01947419668938656\n",
      "\n",
      "Relation: http://dbpedia.org/ontology/writer\n",
      "Best Threshold: 0.787\n",
      "Best Precision: 0.8928571428571429\n",
      "Recall for best Threshold: 0.02478929102627665\n",
      "F1 for best Threshold: 0.0482392667631452\n",
      "\n",
      "Relation: http://dbpedia.org/ontology/subsequentWork\n",
      "Best Threshold: 0.533\n",
      "Best Precision: 0.5829145728643216\n",
      "Recall for best Threshold: 0.20825852782764812\n",
      "F1 for best Threshold: 0.30687830687830686\n",
      "\n",
      "Relation: http://dbpedia.org/ontology/previousWork\n",
      "Best Threshold: 0.614\n",
      "Best Precision: 0.5984703632887189\n",
      "Recall for best Threshold: 0.3220164609053498\n",
      "F1 for best Threshold: 0.41872909698996663\n",
      "\n",
      "Relation: http://dbpedia.org/ontology/artist\n",
      "Best Threshold: 0.886\n",
      "Best Precision: 0.9\n",
      "Recall for best Threshold: 0.013386217154189391\n",
      "F1 for best Threshold: 0.026380068392769906\n",
      "\n",
      "Relation: http://dbpedia.org/ontology/formerTeam\n",
      "Best Threshold: 0.878\n",
      "Best Precision: 0.9285714285714286\n",
      "Recall for best Threshold: 0.006445215666831929\n",
      "F1 for best Threshold: 0.012801575578532742\n",
      "\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "# Initialize a dictionary to store the optimal thresholds for each relation\n",
    "#optimal_thresholds = {}\n",
    "\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialize the Naive Bayes classifier\n",
    "nb_classifier = GaussianNB()\n",
    "\n",
    "# Initialize the RIPPER classifier\n",
    "ripper_classifier = lw.RIPPER()\n",
    "\n",
    "# Initialize the SVM classifier\n",
    "svm_classifier = SVC(kernel='linear', probability=True)\n",
    "\n",
    "classifiers = [(nb_classifier, \"nb\"), (rf_classifier, \"rf\"), (svm_classifier, \"svm\"), (ripper_classifier, \"ripper\")]\n",
    "\n",
    "for classifier, name in classifiers:\n",
    "\n",
    "    print(name)\n",
    "    print()\n",
    "\n",
    "    for index, relation in enumerate(relations):\n",
    "      relation_name = nicos_relations[index]\n",
    "\n",
    "      if name == \"rf\":\n",
    "        classifier = RandomForestClassifier(random_state=42, \n",
    "                                            max_depth=best_hp_rf[index][0],\n",
    "                                            n_estimators=best_hp_rf[index][1])\n",
    "    \n",
    "      X_train, X_test, y_train, y_test = train_test_split(\n",
    "        relation.drop(['domain', 'relation', 'range', 'label'], axis=1),\n",
    "        relation['label'],\n",
    "        test_size=0.2, \n",
    "        random_state=42 \n",
    "      )\n",
    "    \n",
    "      # Balance the training data using RandomOverSampler\n",
    "      oversampler = RandomOverSampler(sampling_strategy=0.4, random_state=42)\n",
    "      #X_train, y_train = oversampler.fit_resample(X_train, y_train)\n",
    "    \n",
    "      classifier.fit(X_train, y_train)\n",
    "\n",
    "      y_pred = classifier.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "      best_precision = 0\n",
    "      best_threshold = 0\n",
    "    \n",
    "      for threshold in range(1, 1000):\n",
    "        threshold /= 1000\n",
    "    \n",
    "        y_pred_binary = (y_pred >= threshold).astype(int)\n",
    "    \n",
    "        precision = precision_score(y_test, y_pred_binary, zero_division=0)\n",
    "    \n",
    "        # Track the best precision and threshold\n",
    "        if precision >= best_precision and precision <= 0.95:\n",
    "          best_precision = precision\n",
    "          best_threshold = threshold\n",
    "    \n",
    "      # Store the optimal threshold for this relation\n",
    "      #optimal_thresholds[relation_name] = best_threshold\n",
    "    \n",
    "      # Apply the best threshold to the entire test set for this relation\n",
    "      relation_y_pred_binary = (y_pred >= best_threshold).astype(int)\n",
    "    \n",
    "      # Calculate overall precision and recall for this relation\n",
    "      overall_recall = recall_score(y_test, relation_y_pred_binary)\n",
    "    \n",
    "      f1 = f1_score(y_test, relation_y_pred_binary)\n",
    "    \n",
    "      # Print the results for this relation\n",
    "      print(f\"Relation: {relation_name}\")\n",
    "      print(f\"Best Threshold: {best_threshold}\")\n",
    "      print(f\"Best Precision: {best_precision}\")\n",
    "      print(f\"Recall for best Threshold: {overall_recall}\")\n",
    "      print(f\"F1 for best Threshold: {f1}\")\n",
    "      print()\n",
    "\n",
    "    print('-------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype):\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype):\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relation: http://dbpedia.org/ontology/birthPlace\n",
      "Best Threshold: 0.951\n",
      "Best Precision: 0.8333333333333334\n",
      "Recall for best Threshold: 0.00495785820525533\n",
      "F1 for best Threshold: 0.009857072449482505\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype):\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype):\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relation: http://dbpedia.org/ontology/family\n",
      "Best Threshold: 0.716\n",
      "Best Precision: 0.9481481481481482\n",
      "Recall for best Threshold: 0.7314285714285714\n",
      "F1 for best Threshold: 0.8258064516129032\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype):\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype):\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relation: http://dbpedia.org/ontology/deathPlace\n",
      "Best Threshold: 0.954\n",
      "Best Precision: 0.9\n",
      "Recall for best Threshold: 0.004462072384729797\n",
      "F1 for best Threshold: 0.008880118401578686\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype):\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype):\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relation: http://dbpedia.org/ontology/producer\n",
      "Best Threshold: 0.845\n",
      "Best Precision: 0.8571428571428571\n",
      "Recall for best Threshold: 0.005928853754940711\n",
      "F1 for best Threshold: 0.011776251226692836\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype):\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype):\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relation: http://dbpedia.org/ontology/writer\n",
      "Best Threshold: 0.916\n",
      "Best Precision: 0.9411764705882353\n",
      "Recall for best Threshold: 0.007932573128408527\n",
      "F1 for best Threshold: 0.01573254670599803\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype):\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype):\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relation: http://dbpedia.org/ontology/subsequentWork\n",
      "Best Threshold: 0.881\n",
      "Best Precision: 0.875\n",
      "Recall for best Threshold: 0.012567324955116697\n",
      "F1 for best Threshold: 0.024778761061946902\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype):\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype):\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relation: http://dbpedia.org/ontology/previousWork\n",
      "Best Threshold: 0.818\n",
      "Best Precision: 0.8780487804878049\n",
      "Recall for best Threshold: 0.037037037037037035\n",
      "F1 for best Threshold: 0.07107601184600196\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype):\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype):\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relation: http://dbpedia.org/ontology/artist\n",
      "Best Threshold: 0.971\n",
      "Best Precision: 0.9393939393939394\n",
      "Recall for best Threshold: 0.015369360436291522\n",
      "F1 for best Threshold: 0.03024390243902439\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype):\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:335: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:338: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  is_categorical_dtype(dtype) or is_pa_ext_categorical_dtype(dtype)\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:384: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype):\n",
      "/pfs/data5/home/ma/ma_ma/ma_nfuerhau/testenv/lib/python3.9/site-packages/xgboost/data.py:359: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relation: http://dbpedia.org/ontology/formerTeam\n",
      "Best Threshold: 0.982\n",
      "Best Precision: 0.9210526315789473\n",
      "Recall for best Threshold: 0.017352503718393655\n",
      "F1 for best Threshold: 0.03406326034063261\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, relation in enumerate(relations):\n",
    "  relation_name = nicos_relations[index]\n",
    "\n",
    "  X_train, X_test, y_train, y_test = train_test_split(\n",
    "    relation.drop(['domain', 'relation', 'range', 'label'], axis=1),\n",
    "    relation['label'],\n",
    "    test_size=0.2,  \n",
    "    random_state=42  \n",
    "    )\n",
    "\n",
    "  # Balance the training data using RandomOverSampler\n",
    "  oversampler = RandomOverSampler(sampling_strategy=0.4, random_state=42)\n",
    "  #X_train_balanced, y_train_balanced = oversampler.fit_resample(X_train, y_train)\n",
    "\n",
    "  # Create a DMatrix for XGBoost\n",
    "  dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "  dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "  # Set hyperparameters\n",
    "  num_round = 200\n",
    "    \n",
    "  params = {\n",
    "      'objective': 'binary:logistic',  # For binary classification\n",
    "      'eval_metric': 'logloss',  # Logarithmic loss\n",
    "      'max_depth': 7,\n",
    "      'eta': 0.2,  # Learning rate\n",
    "      'subsample': 0.7,\n",
    "      'colsample_bytree': 0.7\n",
    "  }  \n",
    "\n",
    "  # Train the model\n",
    "  model = xgb.train(params, dtrain, num_round)\n",
    "\n",
    "  y_pred = model.predict(dtest)\n",
    "\n",
    "  best_precision = 0\n",
    "  best_threshold = 0\n",
    "\n",
    "  for threshold in range(1, 1000):\n",
    "    threshold /= 1000\n",
    "\n",
    "    y_pred_binary = (y_pred >= threshold).astype(int)\n",
    "\n",
    "    precision = precision_score(y_test, y_pred_binary, zero_division=0)\n",
    "\n",
    "    # Track the best precision and threshold\n",
    "    if precision >= best_precision and precision <= 0.95:\n",
    "      best_precision = precision\n",
    "      best_threshold = threshold\n",
    "\n",
    "  # Store the optimal threshold for this relation\n",
    "  #optimal_thresholds[relation_name] = best_threshold\n",
    "\n",
    "  # Apply the best threshold to the entire test set for this relation\n",
    "  relation_y_pred_binary = (y_pred >= best_threshold).astype(int)\n",
    "\n",
    "  # Calculate overall precision and recall for this relation\n",
    "  overall_recall = recall_score(y_test, relation_y_pred_binary)\n",
    "\n",
    "  f1 = f1_score(y_test, relation_y_pred_binary)\n",
    "\n",
    "  # Print the results for this relation\n",
    "  print(f\"Relation: {relation_name}\")\n",
    "  print(f\"Best Threshold: {best_threshold}\")\n",
    "  print(f\"Best Precision: {best_precision}\")\n",
    "  print(f\"Recall for best Threshold: {overall_recall}\")\n",
    "  print(f\"F1 for best Threshold: {f1}\")\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-22 00:44:44.832769: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-22 00:44:45.566929: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-22 00:44:45.566976: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-22 00:44:45.567006: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-22 00:44:45.761962: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-22 00:45:05.219376: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 10s 3ms/step - loss: 0.4282 - binary_accuracy: 0.8052 - val_loss: 0.4139 - val_binary_accuracy: 0.8108\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4193 - binary_accuracy: 0.8088 - val_loss: 0.4135 - val_binary_accuracy: 0.8051\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4145 - binary_accuracy: 0.8091 - val_loss: 0.4080 - val_binary_accuracy: 0.8105\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4119 - binary_accuracy: 0.8123 - val_loss: 0.4086 - val_binary_accuracy: 0.8111\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.4104 - binary_accuracy: 0.8126 - val_loss: 0.4071 - val_binary_accuracy: 0.8125\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 8s 4ms/step - loss: 0.4085 - binary_accuracy: 0.8123 - val_loss: 0.4037 - val_binary_accuracy: 0.8133\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 8s 4ms/step - loss: 0.4074 - binary_accuracy: 0.8124 - val_loss: 0.4041 - val_binary_accuracy: 0.8125\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4059 - binary_accuracy: 0.8136 - val_loss: 0.4044 - val_binary_accuracy: 0.8115\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.4044 - binary_accuracy: 0.8141 - val_loss: 0.4057 - val_binary_accuracy: 0.8110\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4047 - binary_accuracy: 0.8155 - val_loss: 0.4035 - val_binary_accuracy: 0.8127\n",
      "313/313 [==============================] - 1s 2ms/step\n",
      "Relation: http://dbpedia.org/ontology/birthPlace\n",
      "Best Threshold: 0.875\n",
      "Best Precision: 0.8\n",
      "Recall for best Threshold: 0.0039662865642042635\n",
      "F1 for best Threshold: 0.007893438579181055\n",
      "\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 9s 4ms/step - loss: 0.0579 - binary_accuracy: 0.9733 - val_loss: 0.0600 - val_binary_accuracy: 0.9747\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.0480 - binary_accuracy: 0.9790 - val_loss: 0.0475 - val_binary_accuracy: 0.9797\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.0466 - binary_accuracy: 0.9789 - val_loss: 0.0524 - val_binary_accuracy: 0.9801\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.0460 - binary_accuracy: 0.9797 - val_loss: 0.0457 - val_binary_accuracy: 0.9796\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 8s 4ms/step - loss: 0.0446 - binary_accuracy: 0.9803 - val_loss: 0.0477 - val_binary_accuracy: 0.9790\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.0434 - binary_accuracy: 0.9813 - val_loss: 0.0495 - val_binary_accuracy: 0.9795\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.0427 - binary_accuracy: 0.9814 - val_loss: 0.0461 - val_binary_accuracy: 0.9804\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.0419 - binary_accuracy: 0.9817 - val_loss: 0.0491 - val_binary_accuracy: 0.9803\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.0437 - binary_accuracy: 0.9807 - val_loss: 0.0465 - val_binary_accuracy: 0.9804\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.0411 - binary_accuracy: 0.9821 - val_loss: 0.0444 - val_binary_accuracy: 0.9804\n",
      "313/313 [==============================] - 1s 2ms/step\n",
      "Relation: http://dbpedia.org/ontology/family\n",
      "Best Threshold: 0.728\n",
      "Best Precision: 0.9483695652173914\n",
      "Recall for best Threshold: 0.6647619047619048\n",
      "F1 for best Threshold: 0.7816349384098544\n",
      "\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4761 - binary_accuracy: 0.8033 - val_loss: 0.4598 - val_binary_accuracy: 0.8052\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 8s 4ms/step - loss: 0.4640 - binary_accuracy: 0.8042 - val_loss: 0.4631 - val_binary_accuracy: 0.8062\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 8s 4ms/step - loss: 0.4590 - binary_accuracy: 0.8069 - val_loss: 0.4594 - val_binary_accuracy: 0.8080\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4579 - binary_accuracy: 0.8059 - val_loss: 0.4594 - val_binary_accuracy: 0.8071\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.4554 - binary_accuracy: 0.8077 - val_loss: 0.4526 - val_binary_accuracy: 0.8085\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 8s 4ms/step - loss: 0.4548 - binary_accuracy: 0.8082 - val_loss: 0.4554 - val_binary_accuracy: 0.8076\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 8s 4ms/step - loss: 0.4535 - binary_accuracy: 0.8080 - val_loss: 0.4519 - val_binary_accuracy: 0.8098\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 8s 4ms/step - loss: 0.4519 - binary_accuracy: 0.8097 - val_loss: 0.4519 - val_binary_accuracy: 0.8079\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4511 - binary_accuracy: 0.8086 - val_loss: 0.4492 - val_binary_accuracy: 0.8092\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4509 - binary_accuracy: 0.8093 - val_loss: 0.4587 - val_binary_accuracy: 0.8089\n",
      "313/313 [==============================] - 1s 2ms/step\n",
      "Relation: http://dbpedia.org/ontology/deathPlace\n",
      "Best Threshold: 0.758\n",
      "Best Precision: 0.9090909090909091\n",
      "Recall for best Threshold: 0.00495785820525533\n",
      "F1 for best Threshold: 0.009861932938856016\n",
      "\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.2675 - binary_accuracy: 0.8987 - val_loss: 0.2624 - val_binary_accuracy: 0.8984\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.2575 - binary_accuracy: 0.8986 - val_loss: 0.2597 - val_binary_accuracy: 0.8984\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 8s 4ms/step - loss: 0.2541 - binary_accuracy: 0.8982 - val_loss: 0.2572 - val_binary_accuracy: 0.8984\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.2527 - binary_accuracy: 0.8987 - val_loss: 0.2622 - val_binary_accuracy: 0.8984\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.2513 - binary_accuracy: 0.8987 - val_loss: 0.2553 - val_binary_accuracy: 0.8984\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.2508 - binary_accuracy: 0.8987 - val_loss: 0.2555 - val_binary_accuracy: 0.8984\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.2497 - binary_accuracy: 0.8987 - val_loss: 0.2584 - val_binary_accuracy: 0.8984\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.2500 - binary_accuracy: 0.8984 - val_loss: 0.2579 - val_binary_accuracy: 0.8984\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.2500 - binary_accuracy: 0.8988 - val_loss: 0.2603 - val_binary_accuracy: 0.8984\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.2496 - binary_accuracy: 0.8987 - val_loss: 0.2554 - val_binary_accuracy: 0.8984\n",
      "313/313 [==============================] - 1s 2ms/step\n",
      "Relation: http://dbpedia.org/ontology/producer\n",
      "Best Threshold: 0.44\n",
      "Best Precision: 0.5196850393700787\n",
      "Recall for best Threshold: 0.06521739130434782\n",
      "F1 for best Threshold: 0.11589113257243197\n",
      "\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4414 - binary_accuracy: 0.8005 - val_loss: 0.4363 - val_binary_accuracy: 0.8030\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4270 - binary_accuracy: 0.8034 - val_loss: 0.4273 - val_binary_accuracy: 0.8037\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4255 - binary_accuracy: 0.8045 - val_loss: 0.4219 - val_binary_accuracy: 0.8061\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4215 - binary_accuracy: 0.8040 - val_loss: 0.4219 - val_binary_accuracy: 0.8043\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4202 - binary_accuracy: 0.8048 - val_loss: 0.4212 - val_binary_accuracy: 0.8045\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.4188 - binary_accuracy: 0.8060 - val_loss: 0.4238 - val_binary_accuracy: 0.8008\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4184 - binary_accuracy: 0.8067 - val_loss: 0.4210 - val_binary_accuracy: 0.8037\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4169 - binary_accuracy: 0.8075 - val_loss: 0.4245 - val_binary_accuracy: 0.8030\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.4171 - binary_accuracy: 0.8070 - val_loss: 0.4222 - val_binary_accuracy: 0.8043\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.4161 - binary_accuracy: 0.8077 - val_loss: 0.4238 - val_binary_accuracy: 0.8051\n",
      "313/313 [==============================] - 1s 2ms/step\n",
      "Relation: http://dbpedia.org/ontology/writer\n",
      "Best Threshold: 0.64\n",
      "Best Precision: 0.8\n",
      "Recall for best Threshold: 0.029747149231531978\n",
      "F1 for best Threshold: 0.05736137667304015\n",
      "\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.1617 - binary_accuracy: 0.9461 - val_loss: 0.1635 - val_binary_accuracy: 0.9430\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.1545 - binary_accuracy: 0.9447 - val_loss: 0.1668 - val_binary_accuracy: 0.9421\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.1531 - binary_accuracy: 0.9458 - val_loss: 0.1618 - val_binary_accuracy: 0.9433\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.1521 - binary_accuracy: 0.9460 - val_loss: 0.1589 - val_binary_accuracy: 0.9421\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.1519 - binary_accuracy: 0.9465 - val_loss: 0.1582 - val_binary_accuracy: 0.9424\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.1514 - binary_accuracy: 0.9454 - val_loss: 0.1571 - val_binary_accuracy: 0.9430\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.1516 - binary_accuracy: 0.9457 - val_loss: 0.1604 - val_binary_accuracy: 0.9421\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.1509 - binary_accuracy: 0.9458 - val_loss: 0.1571 - val_binary_accuracy: 0.9439\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.1504 - binary_accuracy: 0.9464 - val_loss: 0.1579 - val_binary_accuracy: 0.9438\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 8s 4ms/step - loss: 0.1506 - binary_accuracy: 0.9464 - val_loss: 0.1587 - val_binary_accuracy: 0.9421\n",
      "313/313 [==============================] - 1s 2ms/step\n",
      "Relation: http://dbpedia.org/ontology/subsequentWork\n",
      "Best Threshold: 0.445\n",
      "Best Precision: 0.6\n",
      "Recall for best Threshold: 0.005385996409335727\n",
      "F1 for best Threshold: 0.010676156583629894\n",
      "\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 8s 4ms/step - loss: 0.2401 - binary_accuracy: 0.9143 - val_loss: 0.2403 - val_binary_accuracy: 0.9146\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.2317 - binary_accuracy: 0.9154 - val_loss: 0.2273 - val_binary_accuracy: 0.9162\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.2294 - binary_accuracy: 0.9166 - val_loss: 0.2380 - val_binary_accuracy: 0.9144\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 8s 4ms/step - loss: 0.2281 - binary_accuracy: 0.9163 - val_loss: 0.2370 - val_binary_accuracy: 0.9169\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 8s 4ms/step - loss: 0.2279 - binary_accuracy: 0.9168 - val_loss: 0.2262 - val_binary_accuracy: 0.9166\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.2267 - binary_accuracy: 0.9170 - val_loss: 0.2271 - val_binary_accuracy: 0.9156\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.2271 - binary_accuracy: 0.9168 - val_loss: 0.2270 - val_binary_accuracy: 0.9160\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.2254 - binary_accuracy: 0.9172 - val_loss: 0.2348 - val_binary_accuracy: 0.9169\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.2254 - binary_accuracy: 0.9173 - val_loss: 0.2419 - val_binary_accuracy: 0.9159\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.2251 - binary_accuracy: 0.9175 - val_loss: 0.2269 - val_binary_accuracy: 0.9172\n",
      "313/313 [==============================] - 1s 2ms/step\n",
      "Relation: http://dbpedia.org/ontology/previousWork\n",
      "Best Threshold: 0.678\n",
      "Best Precision: 0.9333333333333333\n",
      "Recall for best Threshold: 0.01440329218106996\n",
      "F1 for best Threshold: 0.028368794326241134\n",
      "\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.3646 - binary_accuracy: 0.8164 - val_loss: 0.3504 - val_binary_accuracy: 0.8235\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.3555 - binary_accuracy: 0.8196 - val_loss: 0.3563 - val_binary_accuracy: 0.8259\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.3510 - binary_accuracy: 0.8202 - val_loss: 0.3547 - val_binary_accuracy: 0.8242\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.3498 - binary_accuracy: 0.8245 - val_loss: 0.3451 - val_binary_accuracy: 0.8311\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 8s 4ms/step - loss: 0.3490 - binary_accuracy: 0.8231 - val_loss: 0.3522 - val_binary_accuracy: 0.8300\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.3470 - binary_accuracy: 0.8249 - val_loss: 0.3447 - val_binary_accuracy: 0.8321\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.3474 - binary_accuracy: 0.8248 - val_loss: 0.3439 - val_binary_accuracy: 0.8314\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.3461 - binary_accuracy: 0.8247 - val_loss: 0.3497 - val_binary_accuracy: 0.8294\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 8s 4ms/step - loss: 0.3454 - binary_accuracy: 0.8237 - val_loss: 0.3448 - val_binary_accuracy: 0.8307\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.3448 - binary_accuracy: 0.8261 - val_loss: 0.3541 - val_binary_accuracy: 0.8304\n",
      "313/313 [==============================] - 1s 2ms/step\n",
      "Relation: http://dbpedia.org/ontology/artist\n",
      "Best Threshold: 0.637\n",
      "Best Precision: 0.8571428571428571\n",
      "Recall for best Threshold: 0.005949429846306396\n",
      "F1 for best Threshold: 0.011816838995568684\n",
      "\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 8s 3ms/step - loss: 0.2345 - binary_accuracy: 0.8687 - val_loss: 0.2248 - val_binary_accuracy: 0.8841\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.2223 - binary_accuracy: 0.8798 - val_loss: 0.2293 - val_binary_accuracy: 0.8700\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 8s 4ms/step - loss: 0.2210 - binary_accuracy: 0.8818 - val_loss: 0.2263 - val_binary_accuracy: 0.8870\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.2194 - binary_accuracy: 0.8838 - val_loss: 0.2124 - val_binary_accuracy: 0.8907\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.2172 - binary_accuracy: 0.8853 - val_loss: 0.2176 - val_binary_accuracy: 0.8876\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.2162 - binary_accuracy: 0.8862 - val_loss: 0.2159 - val_binary_accuracy: 0.8873\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.2165 - binary_accuracy: 0.8862 - val_loss: 0.2093 - val_binary_accuracy: 0.8957\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.2157 - binary_accuracy: 0.8872 - val_loss: 0.2137 - val_binary_accuracy: 0.8915\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.2147 - binary_accuracy: 0.8878 - val_loss: 0.2162 - val_binary_accuracy: 0.8903\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.2146 - binary_accuracy: 0.8882 - val_loss: 0.2144 - val_binary_accuracy: 0.8917\n",
      "313/313 [==============================] - 1s 2ms/step\n",
      "Relation: http://dbpedia.org/ontology/formerTeam\n",
      "Best Threshold: 0.779\n",
      "Best Precision: 0.9230769230769231\n",
      "Recall for best Threshold: 0.023797719385225583\n",
      "F1 for best Threshold: 0.046399226679555344\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, relation in enumerate(relations):\n",
    "  relation_name = nicos_relations[index]\n",
    "\n",
    "  X_train, X_test, y_train, y_test = train_test_split(\n",
    "    relation.drop(['domain', 'relation', 'range', 'label'], axis=1),\n",
    "    relation['label'],\n",
    "    test_size=0.2,  \n",
    "    random_state=42  \n",
    "    )\n",
    "\n",
    "  # Balance the training data using RandomOverSampler\n",
    "  oversampler = RandomOverSampler(sampling_strategy=0.4, random_state=42)\n",
    "  #X_train_balanced, y_train_balanced = oversampler.fit_resample(X_train, y_train)\n",
    "\n",
    "  model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(1024, activation='relu', input_shape=(len(X_train.columns),)),\n",
    "      tf.keras.layers.Dense(512, activation='relu'),\n",
    "      tf.keras.layers.Dense(256, activation='relu'),\n",
    "      tf.keras.layers.Dense(128, activation='relu'),\n",
    "      tf.keras.layers.Dense(64, activation='relu'),\n",
    "      tf.keras.layers.Dense(32, activation='relu'),\n",
    "      tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "  ])\n",
    "\n",
    "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "\n",
    "  # Train the model\n",
    "  model.fit(X_train, y_train, epochs=10, batch_size=16, validation_split=0.2)\n",
    "\n",
    "  # Convert data to NumPy arrays\n",
    "  X_test_np = np.array(X_test)\n",
    "\n",
    "  y_pred = model.predict(X_test_np)\n",
    "\n",
    "  best_precision = 0\n",
    "  best_threshold = 0\n",
    "\n",
    "  for threshold in range(1, 1000):\n",
    "    threshold /= 1000\n",
    "\n",
    "    y_pred_binary = (y_pred >= threshold).astype(int)\n",
    "\n",
    "    precision = precision_score(y_test, y_pred_binary, zero_division=0)\n",
    "\n",
    "    # Track the best precision and threshold\n",
    "    if precision >= best_precision and precision <= 0.95:\n",
    "      best_precision = precision\n",
    "      best_threshold = threshold\n",
    "  # Store the optimal threshold for this relation\n",
    "  #optimal_thresholds[relation_name] = best_threshold\n",
    "\n",
    "  # Apply the best threshold to the entire test set for this relation\n",
    "  relation_y_pred_binary = (y_pred >= best_threshold).astype(int)\n",
    "\n",
    "  # Calculate overall precision and recall for this relation\n",
    "  overall_recall = recall_score(y_test, relation_y_pred_binary)\n",
    "\n",
    "  f1 = f1_score(y_test, relation_y_pred_binary)\n",
    "\n",
    "  # Print the results for this relation\n",
    "  print(f\"Relation: {relation_name}\")\n",
    "  print(f\"Best Threshold: {best_threshold}\")\n",
    "  print(f\"Best Precision: {best_precision}\")\n",
    "  print(f\"Recall for best Threshold: {overall_recall}\")\n",
    "  print(f\"F1 for best Threshold: {f1}\")\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for relation in relations:\n",
    "    columns_to_drop = relation.columns[(relation == 0).all()]\n",
    "    relation.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Random Forest Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://dbpedia.org/ontology/birthPlace\n",
      "Best hyperparameters: {'max_depth': 14, 'n_estimators': 347}\n",
      "http://dbpedia.org/ontology/family\n",
      "Best hyperparameters: {'max_depth': 19, 'n_estimators': 95}\n",
      "http://dbpedia.org/ontology/deathPlace\n",
      "Best hyperparameters: {'max_depth': 7, 'n_estimators': 248}\n",
      "http://dbpedia.org/ontology/producer\n",
      "Best hyperparameters: {'max_depth': 15, 'n_estimators': 248}\n",
      "http://dbpedia.org/ontology/writer\n",
      "Best hyperparameters: {'max_depth': 10, 'n_estimators': 395}\n",
      "http://dbpedia.org/ontology/subsequentWork\n",
      "Best hyperparameters: {'max_depth': 4, 'n_estimators': 57}\n",
      "http://dbpedia.org/ontology/previousWork\n",
      "Best hyperparameters: {'max_depth': 12, 'n_estimators': 99}\n",
      "http://dbpedia.org/ontology/artist\n",
      "Best hyperparameters: {'max_depth': 12, 'n_estimators': 430}\n",
      "http://dbpedia.org/ontology/formerTeam\n",
      "Best hyperparameters: {'max_depth': 11, 'n_estimators': 250}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "for index, relation in enumerate(relations):\n",
    "    relation_name = nicos_relations[index]\n",
    "\n",
    "    print(relation_name)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        relation.drop(['domain', 'relation', 'range', 'label'], axis=1),\n",
    "        relation['label'],\n",
    "        test_size=0.2,  \n",
    "        random_state=42  \n",
    "        )\n",
    "    \n",
    "    param_dist = {'n_estimators': randint(50,500),\n",
    "                  'max_depth': randint(1,20)}\n",
    "\n",
    "    # Create a random forest classifier\n",
    "    rf = RandomForestClassifier()\n",
    "    \n",
    "    #Random search to find the best hyperparameters\n",
    "    rand_search = RandomizedSearchCV(rf, \n",
    "                                     param_distributions = param_dist, \n",
    "                                     n_iter=2, \n",
    "                                     cv=5)\n",
    "    \n",
    "    # Fit the random search object to the data\n",
    "    rand_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Create a variable for the best model\n",
    "    best_rf = rand_search.best_estimator_\n",
    "    \n",
    "    # Print the best hyperparameters\n",
    "    print('Best hyperparameters:',  rand_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hp_rf = [(14, 347), (19, 95), (7, 248), (15, 248), (10, 395), (4, 57), (12, 99), (12, 430), (11, 250)]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
